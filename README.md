# SpeechToText-Prototype7

A React TypeScript prototype application for testing the fal.ai Wizper speech-to-text API integration. This prototype will serve as a foundation for integrating speech-to-text functionality into the Navis application.

## ğŸ¯ Purpose

This prototype is designed to:
- Test the fal.ai Wizper API for speech-to-text conversion
- Evaluate real-time audio capture and processing
- Prototype UI components for transcript display
- Measure performance metrics and accuracy
- Prepare integration patterns for the main Navis application

## ğŸš€ Features

### Phase 1 (Current Implementation)
- **Audio Capture**: Browser-based microphone recording with MediaRecorder API
- **Real-time Processing**: Chunked audio processing for near real-time results
- **fal.ai Wizper Integration**: Direct API integration with error handling and retries
- **Transcript Display**: Real-time transcript with confidence indicators
- **Modern UI**: Clean interface built with React + TypeScript + Tailwind CSS

### Planned Features (Phase 2 & 3)
- Enhanced audio visualization
- Voice Activity Detection (VAD)
- Speaker diarization support
- Performance optimization
- Integration preparation for Navis

## ğŸ› ï¸ Tech Stack

- **Frontend**: React 18 + TypeScript
- **Styling**: Tailwind CSS
- **Build Tool**: Vite
- **Audio API**: MediaRecorder API (Web Audio)
- **Speech-to-Text**: fal.ai Wizper API

## ğŸ“‹ Prerequisites

1. **Node.js**: Version 16 or higher
2. **fal.ai API Key**: You'll need a valid API key from [fal.ai](https://fal.ai/)
3. **Modern Browser**: Chrome, Firefox, or Safari with microphone support

## âš¡ Quick Start

1. **Install Dependencies**
   ```bash
   npm install
   ```

2. **Start Development Server**
   ```bash
   npm run dev
   ```

3. **Open Browser**
   - Navigate to `http://localhost:3000`
   - Allow microphone permissions when prompted

4. **Configure API**
   - Click "Configure API Key" in the interface
   - Enter your fal.ai API key

5. **Test Recording**
   - Click "Start Recording"
   - Speak into your microphone
   - Watch the real-time transcript appear
   - Click "Stop" to finish

## ğŸ¨ Project Structure

```
src/
â”œâ”€â”€ components/
â”‚   â”œâ”€â”€ AudioCapture/
â”‚   â”‚   â”œâ”€â”€ AudioControls.tsx       # Recording controls
â”‚   â”‚   â””â”€â”€ RecordingStatus.tsx     # Status indicators
â”‚   â”œâ”€â”€ Layout/
â”‚   â”‚   â”œâ”€â”€ Header.tsx              # App header
â”‚   â”‚   â””â”€â”€ PrototypeLayout.tsx     # Main layout
â”‚   â””â”€â”€ Transcript/
â”‚       â”œâ”€â”€ TranscriptDisplay.tsx   # Main transcript view
â”‚       â”œâ”€â”€ TranscriptSegment.tsx   # Individual segments
â”‚       â””â”€â”€ ConfidenceIndicator.tsx # Confidence visualization
â”œâ”€â”€ hooks/
â”‚   â”œâ”€â”€ useAudioCapture.ts          # Audio recording logic
â”‚   â””â”€â”€ useTranscription.ts         # API integration logic
â”œâ”€â”€ services/
â”‚   â”œâ”€â”€ audioCapture.ts             # Audio capture service
â”‚   â””â”€â”€ wizperAPI.ts                # fal.ai API client
â”œâ”€â”€ types/
â”‚   â”œâ”€â”€ audio.ts                    # Audio-related types
â”‚   â””â”€â”€ transcript.ts               # Transcript types
â””â”€â”€ utils/
    â”œâ”€â”€ audioFormat.ts              # Audio utilities
    â””â”€â”€ constants.ts                # App constants
```

## ğŸ”§ Configuration

### API Configuration
The app requires a fal.ai API key to function. You can:
- Enter it through the UI prompt
- Set it as an environment variable (future enhancement)

### Audio Settings
Audio capture uses these default settings:
- Sample Rate: 16kHz
- Channels: Mono
- Format: WebM (with fallbacks)
- Chunk Duration: 2 seconds

## ğŸ“Š Performance Metrics

The prototype tracks:
- **Latency**: Time from speech to transcript
- **Accuracy**: Word-level confidence scores
- **Processing Time**: API response times
- **Audio Quality**: Input level monitoring

## ğŸ§ª Testing

### Manual Testing Scenarios
1. **Clear Audio**: Test with clear speech in quiet environment
2. **Background Noise**: Test with various noise levels
3. **Multiple Speakers**: Test speaker transitions
4. **Long Sessions**: Test extended recording periods
5. **Different Accents**: Test with various accents and languages

### Browser Compatibility
- âœ… Chrome 80+
- âœ… Firefox 76+
- âœ… Safari 14+
- âŒ Internet Explorer (not supported)

## ğŸ”— Integration with Navis

This prototype is designed for easy integration into the main Navis application:

### Component Migration
- `AudioControls` â†’ `src/components/livecall/AudioControls.tsx`
- `TranscriptDisplay` â†’ `src/components/livecall/TranscriptArea.tsx`
- `WizperAPI` â†’ `src/services/speechRecognition.ts`

### State Management
- Convert React Context to Redux slices
- Integrate with existing CallStateContext
- Add to global store for persistence

### Type Compatibility
All TypeScript interfaces are designed to match Navis patterns and can be directly imported.

## ğŸš€ Deployment

### Build for Production
```bash
npm run build
```

### Preview Production Build
```bash
npm run preview
```

## ğŸ“ˆ Next Steps

1. **Complete Phase 1 Testing**
2. **Implement Phase 2 Features** (Audio visualization, VAD)
3. **Performance Optimization**
4. **Integration Planning** with Navis team
5. **Production Deployment** strategy

## ğŸ› Known Issues

- Audio chunks may not process in exact real-time on slower connections
- API rate limiting may affect continuous recording
- Some browsers require HTTPS for microphone access

## ğŸ“„ License

This is a prototype application for internal development use.

---

**Created**: August 6, 2025  
**Status**: Phase 1 Implementation Complete  
**Next Review**: After Phase 1 testing completion